{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "304935b8-d57e-436e-bf04-aae035126d13",
   "metadata": {},
   "source": [
    "# CS986 Spotify Classification Problem 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8d77cc-35b8-4b1f-989c-b422b370da5d",
   "metadata": {},
   "source": [
    "### Introduction / Abstract ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f28958",
   "metadata": {},
   "source": [
    "This classification project aimed to predict song genres using machine learning models.\n",
    "The initial process examines the raw dataset, which contains various audio factors and metadata, and proceed with data cleaning — filling missing values, removing outliers, and merging rare classes to avoid stratification issues. Feature engineering plays a vital role in uncovering hidden relationships; for example, we derive “song_age,” “energy_loudness,” and others captures a track’s unique characteristics. We then address class imbalances using SMOTE, ensuring the models does not overlook minority genres.\n",
    "\n",
    "Moreover, we implement baseline models —such as Logistic Regression, Decision Trees, and k-Nearest Neighbors to establish an initial benchmark. Building on these foundations, we explore advanced algorithms, including Random Forests, Support Vector Machines, and gradient boosting frameworks (XGBoost, CatBoost). Finally, we employ ensemble methods, such as Voting and Stacking, that combine the strengths of multiple models. Each step’s performance is assessed with key metrics (accuracy, F1 score), guiding model selection and refinement. Based on the aforementioned metrics, the final models were chosen: k-NN (baseline) & Voting Classification (advanced).\n",
    "\n",
    "Our score is evaluated in Kaggle Leaderboards:\n",
    "- k-NN (0.03)\n",
    "- Voting Classifier (0.44)\n",
    "\n",
    "Overall, this notebook demonstrates the value of preprocessing, feature engineering, and experimentation with diverse algorithms can significantly improve genre classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8730f5",
   "metadata": {},
   "source": [
    "#### Library ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfc62d9-9a09-4505-892c-10d59df6690b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12c854ef-c6d0-4407-9789-528840da9fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labeled_raw = pd.read_csv('CS98XClassificationTrain.csv')\n",
    "df_unlabeled_raw = pd.read_csv('CS98XClassificationTest.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27e097c",
   "metadata": {},
   "source": [
    "Two datasets were utilised for this task. The training dataset has 453 rows & 15 columns, including the target variable 'top genre'. The testing has 113 rows & 14 columns, excluding the 'top genre' column. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b49471b-7597-4af7-a07c-1f684a45f4ed",
   "metadata": {},
   "source": [
    " ### 1.0 Data Preprocessing & Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac725e0-7607-43d3-98c3-f16a38fa5916",
   "metadata": {},
   "source": [
    "#### 1.1 Handling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4204991e",
   "metadata": {},
   "source": [
    "Top Genre: 15 null values was found during data exploration in the training dataset, these values have been manually filled based on research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d89f2b8f-cde5-481a-8b73-8f1fc8cae318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually fill missing values based on research:\n",
    "genre_mapping = {\n",
    "    'Unchained Melody': 'pop', 'Someone Elses Roses': 'adult standards', 'Drinks On The House': 'jazz', 'Pachuko Hop': 'blues',\n",
    "    'Little Things Means A Lot': 'pop', 'The Lady Is A Tramp': 'jazz', 'If I Give My Heart To You': 'pop', 'Happy Days And Lonely Nights': 'pop',\n",
    "    'Stairway Of Love': 'pop', 'You': 'pop', 'No Other Love': 'pop', \"I've Waited So Long\": 'pop', 'Hot Diggity': 'pop', \n",
    "    'Ain\\'t That Just the Way': 'pop', 'I Promised Myself': 'pop'}\n",
    "\n",
    "# Update the 'top genre' column\n",
    "df_labeled_raw['top genre'] = df_labeled_raw['top genre'].fillna(df_labeled_raw['title'].map(genre_mapping))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215cd00d-6c8f-449a-8185-bf66fdfbbe46",
   "metadata": {},
   "source": [
    "#### 1.2 Handling Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8b5fee1-1052-4147-a032-04a8419a7cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(df_labeled_raw.duplicated().sum())\n",
    "print(df_unlabeled_raw.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ad899c-09d0-48f6-ad19-06302bdc4c88",
   "metadata": {},
   "source": [
    "#### 1.3 Encoding Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28ac5f8-888f-40bb-811b-a7e077566f54",
   "metadata": {},
   "source": [
    "- Low-cardinality categorical features: Use One-Hot Encoding (OHE).\n",
    "- High-cardinality categorical features: Use Target Encoding, Frequency Encoding, or Embedding Techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f65cca54-c779-4f28-ad40-5d1010730d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: top genre, Unique Categories: 87\n",
      "Feature: artist, Unique Categories: 345\n"
     ]
    }
   ],
   "source": [
    "# Select categorical columns (assuming 'top genre' and 'artist' are categorical)\n",
    "categorical_columns = [\"top genre\", \"artist\"]\n",
    "\n",
    "# Count unique values in each categorical feature\n",
    "for col in categorical_columns:\n",
    "    unique_values = df_labeled_raw[col].nunique()\n",
    "    print(f\"Feature: {col}, Unique Categories: {unique_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8e1cae",
   "metadata": {},
   "source": [
    "- 87 unique genres (\"top genre\") --> High-cardinality target (multi-class classification).\n",
    "- 345 unique artists (\"artist\") --> Moderate-cardinality categorical feature.\n",
    "\n",
    "With so many unique values, we call them “high cardinality” (for genre) and “moderate cardinality” (for artist). Some machine learning models (Tree-based models: Random Forest, XGBoost) can handle lots of categories better than others.\n",
    "\n",
    "To make things simpler for the algorithms, we convert these text labels into numbers. For the top genre column, we use Label Encoding, which means each genre label gets a unique integer ID. For the artist column, we use One-Hot Encoding, which creates a set of new columns—one for each possible artist—marked with 0s and 1s. This way, the model can process all our data as numeric features instead of text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aef52c7-2a13-49d3-9b4d-f35ce864381f",
   "metadata": {},
   "source": [
    "#### 1.4 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a983045c",
   "metadata": {},
   "source": [
    "We drop Id and title because they don’t help predict a song’s genre. Id is a unique identifier, and title doesn’t capture meaningful musical information. Retaining them would clutter the model with irrelevant details and risk overfitting to specific song names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ac79bc6-649c-43bd-aae1-337129b32762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Irrelevant Columns\n",
    "df_labeled = df_labeled_raw.drop(columns=[\"Id\", \"title\"])\n",
    "df_unlabeled = df_unlabeled_raw.drop(columns=[\"Id\", \"title\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeda956",
   "metadata": {},
   "source": [
    "We created new features from existing song data to help the model understand additional relationships. For instance, converting duration from seconds to minutes, or computing “song_age” from 2025 minus the release year. We also compute loudness or energy ratios that may better capture relevant aspects for modeling. \n",
    "Infinite values (which can result from divisions by zero) are replaced with missing values (NaN). \n",
    "We filled missing numeric values with the median of each column, ensuring we do not lose data rows. This approach helps maintain consistent inputs for downstream modeling while avoiding anomalies or incomplete entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53751659-3ddb-4e5b-bce9-ca0e4807d118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating New Features\n",
    "df_labeled['song_age'] = 2025 - df_labeled['year']\n",
    "df_unlabeled['song_age'] = 2025 - df_unlabeled['year']\n",
    "df_labeled[\"dur_minutes\"] = df_labeled[\"dur\"] / 60\n",
    "df_unlabeled[\"dur_minutes\"] = df_unlabeled[\"dur\"] / 60\n",
    "df_labeled[\"loudness_per_sec\"] = df_labeled[\"dB\"] / df_labeled[\"dur\"]\n",
    "df_unlabeled[\"loudness_per_sec\"] = df_unlabeled[\"dB\"] / df_unlabeled[\"dur\"]\n",
    "df_labeled[\"energy_dance_ratio\"] = df_labeled[\"nrgy\"] / (df_labeled[\"dnce\"] + 1)\n",
    "df_unlabeled[\"energy_dance_ratio\"] = df_unlabeled[\"nrgy\"] / (df_unlabeled[\"dnce\"] + 1)\n",
    "df_labeled[\"acous_per_loud\"] = df_labeled[\"acous\"] / (df_labeled[\"dB\"] + 1)\n",
    "df_unlabeled[\"acous_per_loud\"] = df_unlabeled[\"acous\"] / (df_unlabeled[\"dB\"] + 1)\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_labeled.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_unlabeled.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values only for numerical columns\n",
    "numeric_cols = df_labeled.select_dtypes(include=np.number).columns\n",
    "df_labeled[numeric_cols] = df_labeled[numeric_cols].fillna(df_labeled[numeric_cols].median())\n",
    "\n",
    "numeric_cols_2 = df_unlabeled.select_dtypes(include=np.number).columns\n",
    "df_unlabeled[numeric_cols_2] = df_unlabeled[numeric_cols_2].fillna(df_unlabeled[numeric_cols_2].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff5a5f1",
   "metadata": {},
   "source": [
    "We apply a PowerTransformer (Yeo-Johnson) to selected numeric features so they better follow a normal (bell-shaped) distribution. This handles positive and negative values, reducing skew and stabilising variance. Fitting on the labeled dataset learns appropriate transformations, and then applying the same transformation to both datasets keeps their distributions consistent. This aims to make features more balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc6162e1-3a62-4771-b885-aa1cd0db91d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# Select features to transform\n",
    "features_to_transform = ['year', 'bpm', 'nrgy', 'dnce', 'dB', 'live', 'val', 'dur', 'acous', 'spch', 'pop', 'song_age', 'dur_minutes',\n",
    "                         'loudness_per_sec', 'energy_dance_ratio', 'acous_per_loud']\n",
    "\n",
    "# Initialize PowerTransformer (using yeo-johnson method, which works for both positive and negative values)\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "\n",
    "# Apply PowerTransformer \n",
    "df_labeled[features_to_transform] = pt.fit_transform(df_labeled[features_to_transform]) # (labeled data)\n",
    "df_unlabeled[features_to_transform] = pt.transform(df_unlabeled[features_to_transform]) # (unlabeled data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd7ed82",
   "metadata": {},
   "source": [
    "OneHotEncoder transforms “artist” into several columns (one per unique artist), using 0s and 1s to indicate each row’s artist. We first “fit” on the labeled set so the encoder learns all possible categories, then apply the same transformation to unlabeled data for consistency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6712ee03-41bd-4d32-8e32-f6f72f3eb71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the feature columns -> Encoding with One-Hot Encoding\n",
    "# One-Hot Encoding\n",
    "col_category = [\"artist\"] # Select category feature \n",
    "\n",
    "# Set OneHotEncoder by using the same categories\n",
    "encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "\n",
    "# Fit & Transform\n",
    "encoded_labeled = encoder.fit_transform(df_labeled[col_category])   \n",
    "encoded_unlabeled = encoder.transform(df_unlabeled[col_category])      \n",
    "encoded_labeled_df = pd.DataFrame(encoded_labeled, columns=encoder.get_feature_names_out())      \n",
    "encoded_unlabeled_df = pd.DataFrame(encoded_unlabeled, columns=encoder.get_feature_names_out()) \n",
    "\n",
    "# Handling with numeric columns \n",
    "col_numeric = [\"bpm\", \"nrgy\", \"dnce\", \"live\", \"val\", \"acous\", \"spch\" , \"pop\", \n",
    "               \"song_age\", \"dur_minutes\", \"loudness_per_sec\", \"energy_dance_ratio\", \"acous_per_loud\"]\n",
    "df_labeled_numeric = df_labeled[col_numeric]\n",
    "df_unlabeled_numeric = df_unlabeled[col_numeric]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4467fd98-db4c-4088-8d58-39c1af9d0320",
   "metadata": {},
   "source": [
    "#### 1.5 Standardisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb30af8",
   "metadata": {},
   "source": [
    "Numeric columns were standardised. The scaled numeric data is combined with encoded categorical data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "58568033-04ce-4631-a401-49a6f33dd1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation (Standardisation)\n",
    "scaler = StandardScaler()\n",
    "df_labeled_numeric = pd.DataFrame(scaler.fit_transform(df_labeled_numeric), columns=col_numeric)\n",
    "df_unlabeled_numeric = pd.DataFrame(scaler.transform(df_unlabeled_numeric), columns=col_numeric)\n",
    "\n",
    "# Combine Numeric and Category dataset\n",
    "merged_labeled_df = pd.concat([df_labeled_numeric, encoded_labeled_df], axis=1)\n",
    "merged_unlabeled_df = pd.concat([df_unlabeled_numeric, encoded_unlabeled_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0545a1f-1f75-4059-a758-f1ab6fa455df",
   "metadata": {},
   "source": [
    "#### 1.6 Handling Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26238556",
   "metadata": {},
   "source": [
    "We address imbalanced genres by replacing extremely rare categories with “Unknown,” then label-encode the target. Using SMOTE, we oversample minority classes for a more balanced dataset. Finally, we split the data into training and test sets, ensuring fair evaluation and improving the model’s ability to handle underrepresented genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "65698ce4-aae8-4c17-b8ca-15dc24fb0039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate Features and Target\n",
    "y_train_labeled = df_labeled[\"top genre\"]\n",
    "X_train_labeled = merged_labeled_df\n",
    "X_train_unlabeled = merged_unlabeled_df.copy() \n",
    "\n",
    "# Find categories that appear only 1\n",
    "rare_categories = y_train_labeled.value_counts().loc[lambda x: x == 1].index\n",
    "\n",
    "# Replace them with 'Other'\n",
    "y_train_labeled = y_train_labeled.replace(rare_categories, 'Unknown')\n",
    "\n",
    "# For the target columns -> Encoding with LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train_labeled_encoded = le.fit_transform(y_train_labeled)\n",
    "\n",
    "# Use SMOTE for oversampling target to addressing imbalance target\n",
    "smote = SMOTE(sampling_strategy='auto', k_neighbors=1, random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_labeled, y_train_labeled_encoded)\n",
    "\n",
    "# Split train/test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_resampled, y_train_resampled, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ec247c-dcdc-4275-abbe-25b709522049",
   "metadata": {},
   "source": [
    "## 2.0 Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04a7746-cca9-44d0-b9f7-1d23cdc425d5",
   "metadata": {},
   "source": [
    "Filter Methods (e.g., correlation, mutual information, chi-square test) - \n",
    "Perform standardisation after feature selection because these methods work independently of the model and are not affected by scaling.\n",
    "\n",
    "Wrapper & Embedded Methods (e.g., Recursive Feature Elimination (RFE), Lasso, Decision Trees, Random Forests) - \n",
    "Perform standardisation before feature selection when using models sensitive to feature scales (e.g., logistic regression, SVM, k-NN).\n",
    "If using tree-based models (which are scale-invariant), standardisation is not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "82c2bee2-ad19-4848-bdc5-312346b3d8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import RFE, mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression, LassoCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Dictionary to store selected features for each method\n",
    "selected_features = {}\n",
    "\n",
    "# --- Feature Selection Methods ---\n",
    "# Recursive Feature Elimination (RFE) with Logistic Regression\n",
    "logreg = LogisticRegression(max_iter=500, random_state=42)\n",
    "rfe = RFE(estimator=logreg, n_features_to_select=10)  # Adjust number of features\n",
    "rfe.fit(X_train, y_train)\n",
    "selected_features[\"RFE\"] = X_train.columns[rfe.ranking_ <= np.mean(rfe.ranking_)].tolist()\n",
    "\n",
    "# Lasso for Feature Selection\n",
    "lasso = LassoCV(cv=5, random_state=42, max_iter=5000, tol=1e-4).fit(X_train, y_train)\n",
    "lasso_coef = np.abs(lasso.coef_) \n",
    "selected_features[\"Lasso\"] = X_train.columns[lasso_coef > np.mean(lasso_coef)].tolist()\n",
    "\n",
    "# Tree-Based Feature Selection (Random Forest)\n",
    "random_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances and select features with importance greater than mean\n",
    "feature_importances = np.array(random_forest.feature_importances_)\n",
    "selected_features[\"RandomForest\"] = X_train.columns[feature_importances > np.mean(feature_importances)].tolist()\n",
    "\n",
    "# Mutual Information for Feature Selection\n",
    "mutual_info = mutual_info_classif(X_train, y_train)\n",
    "mutual_info_series = pd.Series(mutual_info, index=X_train.columns)\n",
    "selected_features[\"Mutual Information\"] = mutual_info_series[mutual_info_series > mutual_info_series.mean()].index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51a0e00-aa62-45fe-98c9-7c8517ce3e3b",
   "metadata": {},
   "source": [
    "Now we have different sets of selected features from multiple methods, the next step is evaluating which selection works best for the classification task.\n",
    "Train & Evaluate Models on each feature Set - compare performance metrics (F1-score for imbalance target) on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2102ac5e-7098-4c15-a1d7-3f62873b921a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               RFE  RandomForest  Mutual Information   Lasso\n",
      "F1 Score  0.950053      0.942424             0.93746  0.5548\n"
     ]
    }
   ],
   "source": [
    "# --- Model Evaluation ---\n",
    "def evaluate_feature_set(feature_name, X_train, X_test, y_train, y_test):\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    metrics = {\"F1 Score\": f1_score(y_test, y_pred, average='weighted')}    \n",
    "    return metrics\n",
    "\n",
    "# Evaluate each feature selection method and store the results\n",
    "evaluation_results = {}\n",
    "for method, features in selected_features.items():\n",
    "    X_train_selected = X_train[features]\n",
    "    X_test_selected = X_test[features]\n",
    "    evaluation_results[method] = evaluate_feature_set(method, X_train_selected, X_test_selected, y_train, y_test)\n",
    "\n",
    "# Convert evaluation results to DataFrame for easy readability\n",
    "evaluation_df = pd.DataFrame(evaluation_results).T\n",
    "evaluation_df = evaluation_df.sort_values(by=\"F1 Score\", ascending=False)\n",
    "\n",
    "print(evaluation_df.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace44367-7a7b-4a7e-b057-01f4fb7de0c8",
   "metadata": {},
   "source": [
    "## 3.0 Train & Test Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99752e2f-2614-4366-8362-b32321ba33a9",
   "metadata": {},
   "source": [
    "Select RFE-Selected Features for train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "970c7788-9b98-49d6-ab9f-b8ef680f6288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be6842b",
   "metadata": {},
   "source": [
    "We use RFE to find the most important features and use these to train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8950cddc-a5e6-4e4a-8160-fd3ce96348c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Feature Count: 179\n"
     ]
    }
   ],
   "source": [
    "best_features = selected_features['RFE']\n",
    "\n",
    "# Keep only the selected features\n",
    "X_train_rfe = X_train[best_features]\n",
    "X_val_rfe = X_val[best_features]\n",
    "X_test_rfe = X_test[best_features]\n",
    "print(\"Final Feature Count:\", len(best_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "539af033-62d8-4589-8086-c0638e16ccdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute F1 score\n",
    "def get_metrics(y_true, y_pred):\n",
    "    return {\"F1 Score\": f1_score(y_true, y_pred, average='weighted')}\n",
    "\n",
    "# Function to evaluate models\n",
    "def evaluate_model(model, model_name, X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Get performance for each dataset\n",
    "    train_f1 = get_metrics(y_train, y_train_pred)\n",
    "    val_f1 = get_metrics(y_val, y_val_pred)\n",
    "    test_f1 = get_metrics(y_test, y_test_pred)\n",
    "\n",
    "    # Print results\n",
    "    results_df = pd.DataFrame({\"Train\": train_f1, \"Validation\": val_f1, \"Test\": test_f1})\n",
    "    print(f\"Results for {model_name}:\\n{results_df}\\n\")\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f93e32a-1ca6-47a2-9679-cf1d2e00d5a6",
   "metadata": {},
   "source": [
    "#### 3.1 Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df22009",
   "metadata": {},
   "source": [
    "We define three baseline models—Logistic Regression, Decision Tree, and k-Nearest Neighbors — each capturing different learning patterns: logistic regression for linear decision boundaries, decision trees for non-linear splits, and k-NN for instance-based learning. We found these three reveals which approach suits our data best before deeper refinement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "23c8c1b9-3360-4c2e-a125-d9f9de771eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Baseline Models Evaluation =====\n",
      "\n",
      "Results for Logistic Regression:\n",
      "             Train  Validation      Test\n",
      "F1 Score  0.837198    0.811735  0.815444\n",
      "\n",
      "Results for Decision Tree:\n",
      "            Train  Validation      Test\n",
      "F1 Score  0.99639    0.932091  0.910157\n",
      "\n",
      "Results for K-Nearest Neighbors:\n",
      "             Train  Validation      Test\n",
      "F1 Score  0.994129    0.954786  0.944397\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define baseline models\n",
    "baseline_models = {\n",
    "    \"Logistic Regression\": LogisticRegression(C=0.1, max_iter=500, random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(max_depth=70, random_state=42),\n",
    "    \"K-Nearest Neighbors\": Pipeline([(\"scaler\", StandardScaler()),(\"classifier\", KNeighborsClassifier(n_neighbors=2))])}\n",
    "\n",
    "# Evaluate Baseline Models\n",
    "baseline_results = {}\n",
    "print(\"\\n===== Baseline Models Evaluation =====\\n\")\n",
    "for name, model in baseline_models.items():\n",
    "    baseline_results[name] = evaluate_model(model, name, X_train, X_val, X_test, y_train, y_val, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57e2e3f",
   "metadata": {},
   "source": [
    "k-NN performed the best in terms of Train, Validation and Test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8497fbcc-da2e-4d82-ae78-e8e3010db084",
   "metadata": {},
   "source": [
    "#### 3.2 Advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce60d96",
   "metadata": {},
   "source": [
    "We define advanced models—RandomForest, SVM, XGBoost, CatBoost, and ensembles (Voting, Stacking). Each method captures data patterns differently: tree ensembles learn complex interactions, SVM focuses on robust margins, boosting refines errors at each stage, and ensemble techniques combine strengths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b188af23-d76d-4376-bbd2-8dcbfe0b3276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Advanced Models Evaluation =====\n",
      "\n",
      "Results for Random Forest:\n",
      "             Train  Validation     Test\n",
      "F1 Score  0.915526    0.884651  0.86377\n",
      "\n",
      "Results for SVM:\n",
      "             Train  Validation      Test\n",
      "F1 Score  0.959183    0.917354  0.905048\n",
      "\n",
      "Results for XGBoost:\n",
      "             Train  Validation      Test\n",
      "F1 Score  0.999537    0.929351  0.925939\n",
      "\n",
      "Results for CatBoost:\n",
      "             Train  Validation      Test\n",
      "F1 Score  0.959164    0.902142  0.904008\n",
      "\n",
      "Results for Stacking Classifier:\n",
      "             Train  Validation      Test\n",
      "F1 Score  0.999079    0.945574  0.936981\n",
      "\n",
      "Results for Voting Classifier:\n",
      "             Train  Validation      Test\n",
      "F1 Score  0.990571    0.949492  0.938034\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Define advanced models\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "svm_model = SVC(kernel='poly', C=1, probability=True, random_state=42)\n",
    "xgb_model = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, subsample=0.8, colsample_bytree=0.8, random_state=42)\n",
    "catboost_model = CatBoostClassifier(iterations=100, learning_rate=0.2, depth=6, verbose=0, random_state=42)\n",
    "stacking_model = StackingClassifier(estimators=[('RandomForest', rf_model), ('SVM', svm_model),('XGBoost', xgb_model)], \n",
    "                                    final_estimator=LogisticRegression(max_iter=500, random_state=42))\n",
    "voting_model = VotingClassifier(estimators=[('RandomForest', rf_model), ('SVM', svm_model), ('Catboost', catboost_model)], \n",
    "                                voting='soft') \n",
    "\n",
    "# Advanced Models Dictionary\n",
    "advanced_models = {\"Random Forest\": rf_model, \"SVM\": svm_model, \"XGBoost\": xgb_model,\n",
    "                   \"CatBoost\": catboost_model, \"Stacking Classifier\": stacking_model, \"Voting Classifier\": voting_model}\n",
    "\n",
    "# Evaluate Advanced Models\n",
    "advanced_results = {}\n",
    "print(\"\\n===== Advanced Models Evaluation =====\\n\")\n",
    "for name, model in advanced_models.items():\n",
    "    advanced_results[name] = evaluate_model(model, name, X_train, X_val, X_test, y_train, y_val, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60660a8f",
   "metadata": {},
   "source": [
    "Stacking and Voting Classifier was close, but we opted for Voting Classifier due to the upper edge in Train and Validation.\n",
    "The Voting Classifier outperformed the other five by combining outputs from RandomForest, SVM, XGBoost, CatBoost, and potentially stacking - it reduced each model’s weaknesses. This approach was more consistent and effective than relying on any single model alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e04e78-e90d-4ac0-aa92-4b49b056e9f2",
   "metadata": {},
   "source": [
    "#### 3.3 Refine Top Performing Models by Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a8fa22-47fe-4f8a-899f-8ad26b546ea9",
   "metadata": {},
   "source": [
    "Tried to tune hyperparameters by modifying hyperparameters with different models (for basic and advanced models). As a result, the best model for basic is K-NN, and the best model for advanced is the  Voting Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2621988c-162b-4e0d-8dd6-84323d8b8465",
   "metadata": {},
   "source": [
    "#### 3.4 Voting Classifier is the best Advanced Model (Using K-Fold Cross-Validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d314d018-416c-4f6f-991e-6e07cc72cf43",
   "metadata": {},
   "source": [
    "Created best model and used cross-validation for robust performance estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "468aa9c2-6c0b-48f1-b55b-311a1e2ba22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Cross-Validation Mean F1                          Train F1  \\\n",
      "0                  0.932643  {'F1 Score': 0.9868092405445982}   \n",
      "\n",
      "                      Validation F1                           Test F1  \n",
      "0  {'F1 Score': 0.9479572080114671}  {'F1 Score': 0.9296789699712937}  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "# Define models\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "svm_model = SVC(kernel='poly', C=1, probability=True, random_state=42)\n",
    "catboost_model = CatBoostClassifier(iterations=100, learning_rate=0.2, depth=6, verbose=0, random_state=42)\n",
    "\n",
    "# Create Voting Classifier\n",
    "best_voting_model = VotingClassifier(estimators=[('RandomForest', rf_model), ('SVM', svm_model), ('Catboost', catboost_model)], voting='soft')\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on the training set\n",
    "cv_scores = cross_val_score(best_voting_model, X_train_rfe, y_train, cv=kf, scoring='f1_weighted')\n",
    "\n",
    "# Train the model on the full training set\n",
    "best_voting_model.fit(X_train_rfe, y_train)\n",
    "\n",
    "# Evaluate on validation and test sets\n",
    "y_val_pred = best_voting_model.predict(X_val_rfe)\n",
    "y_test_pred = best_voting_model.predict(X_test_rfe)\n",
    "y_train_pred = best_voting_model.predict(X_train_rfe)\n",
    "\n",
    "# Get performance for each dataset\n",
    "train_f1 = get_metrics(y_train, y_train_pred)\n",
    "val_f1 = get_metrics(y_val, y_val_pred)\n",
    "test_f1 = get_metrics(y_test, y_test_pred)\n",
    "\n",
    "results_df = pd.DataFrame({\"Cross-Validation Mean F1\": [np.mean(cv_scores)], \"Train F1\": [train_f1], \n",
    "                           \"Validation F1\": [val_f1], \"Test F1\": [test_f1]})\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3066aa17-301a-477e-b01f-4af7ef6a9530",
   "metadata": {},
   "source": [
    "## 4.0 Final Model Selection and Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5287b29",
   "metadata": {},
   "source": [
    "#### 4.1 k-NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e46d7e",
   "metadata": {},
   "source": [
    "k-NN is the best model in terms of Train, Validation and Test.\n",
    "Among the baseline models, k-NN performed best because it uses distances between data points to classify songs.\n",
    "That approach aligned well with this dataset’s feature spread, letting k-NN find good neighbors more accurately than the simpler linear (Logistic Regression) or split-based (Decision Tree) method.\n",
    "This can serve as a good starting point as it shows a high F1-score and validity - predicting the song's genres while minimising overfitting. However, it may not overall have an overview of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "92f05c6c-ce23-4c2f-9e55-1894c614b8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model  = Pipeline([(\"scaler\", StandardScaler()),(\"classifier\", KNeighborsClassifier(n_neighbors=2))])\n",
    "\n",
    "# Train \n",
    "knn_model.fit(X_train_rfe, y_train)\n",
    "\n",
    "# Export the predictions for the unlabeled data\n",
    "X_unlabeled_final = X_train_unlabeled[best_features]\n",
    "y_unlabeled_pred_knn = knn_model.predict(X_unlabeled_final)\n",
    "y_unlabeled_labels_knn = le.inverse_transform(y_unlabeled_pred_knn)\n",
    "\n",
    "# Save KNN predictions to CSV\n",
    "test_pred_knn = pd.DataFrame({'Id': df_unlabeled_raw['Id'], 'top genre': y_unlabeled_labels_knn})\n",
    "test_pred_knn.to_csv(\"Predicted_Genres_KNN.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef27c49-8df3-4f6e-b27c-419238e5461a",
   "metadata": {},
   "source": [
    "#### 4.2 Voting\n",
    "\n",
    "Ultimately, the final advanced model was selected. After evaluating multiple advanced models, the Voting Classifier was chosen as the final model due to its superior performance across training, validation, and test sets. The Voting Classifier combines Random Forest, Support Vector Machine (SVM), and XGBoost in a soft-voting strategy, by taking each model's prediction into account and producing an overall \"vote\". This also takes the strengths of each model while offsetting any single model’s weaknesses. This is because each algorithm specialises in different decision boundaries or features of the data, the combined result tends to be more stable and accurate than relying solely on a single classifier’s output.\n",
    "\n",
    "The model was optimised using Stratified K-Fold Cross-Validation (K=5), ensuring that performance estimates were reliable and not biased by a single train-test split. The cross-validation mean F1 score was 0.9326, demonstrating strong consistency across different data folds. After training on the full dataset, the model achieved an F1 score of 0.9868 on the training set, 0.9479 on the validation set, and 0.9297 on the test set. This indicates that the model maintains high generalisation capability without severe overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "abf7bb46-f7f7-47ab-8d5d-04e04fe91f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the predictions for the unlabeled data\n",
    "X_unlabeled_final = X_train_unlabeled[best_features]\n",
    "y_unlabeled_pred_voting = best_voting_model.predict(X_unlabeled_final)\n",
    "y_unlabeled_labels_voting = le.inverse_transform(y_unlabeled_pred_voting)\n",
    "\n",
    "# Save KNN predictions to CSV\n",
    "test_pred_voting = pd.DataFrame({'Id': df_unlabeled_raw['Id'], 'top genre': y_unlabeled_labels_voting})\n",
    "test_pred_voting.to_csv(\"Predicted_Genres_voting.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d351e18c",
   "metadata": {},
   "source": [
    "## 5.0 Kaggle "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33e07f2",
   "metadata": {},
   "source": [
    "On the Kaggle Leaderboards, the baseline model (k-NN) produced a score of 0.035. The chosen advanced model, Voting Classifier severely outperformed the baseline model, with a score of 0.44. This showcases that our model has an effective ability to predict the genre of the songs in the test dataset. The current model had successfully captured behaviours and patterns from the training dataset, further indicating that our feature selection and pre-processing was effective. However, the prediction score from the Kaggle leaderboards is based on 50% on the test data, and final results will be based on the other 50%. The final result of 0.44 could vary as the entrie dataset was not used. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eed3976-3852-457a-90c0-ffb57f3b699c",
   "metadata": {},
   "source": [
    "## 6.0 Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbe2b83-b4cb-4b2b-9de0-4cc7ecb0246f",
   "metadata": {},
   "source": [
    "To further enhance model performance and reduce the gap between training, validation, and test scores, the hyperparameter tuning should be optimised using techniques like grid search or Bayesian optimization to fine-tune parameters for each model in the Voting Classifier, ensuring a balance between bias and variance. This includes adjusting tree depth and estimators in Random Forest and XGBoost, as well as kernel selection in SVM. Moreover, feature selection and engineering should be applied to remove irrelevant or redundant features using techniques like PCA, helping the model focus on the most influential variables while avoiding unnecessary complexity. Lastly, regularisation techniques such as L1/L2 penalties, pruning for tree-based models, and early stopping in XGBoost can be implemented to prevent overfitting and improve generalisation. These strategies will help create a more robust model with consistent performance across different datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcd6a4a-e000-4a77-b93a-564d523b74b9",
   "metadata": {},
   "source": [
    "## 7.0 Concluding Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38c42cc-b725-4f35-8bb0-41d3974f3254",
   "metadata": {},
   "source": [
    "Overall, the Voting Classifier was the best model created to predict the top genre that a song belongs to. With different models created ranging from simple to advanced ones, the Voting Classifier stood out as the best algorithm with the most consistent F1 scores for all the train, validation, and test sets. The K-nearest neighbour model proved to be the most effective out of the baseline models, however, it could not compete with the voting classifier with its effective handling of complex data and flexibility. As well as the actual model building, different feature engineering methods were utilised such as one hot encoding to convert categorical data into a numerical format and power transformer to help stabilise variances. \n",
    "\n",
    "The model was further able to fit the Kaggle dataset very well with a strong score of 0.44, suggesting it  was able to successfully capture  unknown patterns and trends. However as previously mentioned, it is important to note that the Kaggle dataset is based on only 50% of the test data, therefore introducing the possibility of a skewed result. Nonetheless, this report produced an impressive classifier model to help predict the top genre of a song. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
