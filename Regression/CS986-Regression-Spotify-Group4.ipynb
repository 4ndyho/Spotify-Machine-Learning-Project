{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04eab2e6-4d17-4455-b195-67439f3a5208",
   "metadata": {},
   "source": [
    "# CS986 Spotify Regression Problem 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a589af1-1631-4e45-b26d-24d6ea6a0b78",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c909bda8-bc08-4c45-8ca2-e78b937d08b6",
   "metadata": {},
   "source": [
    "In this project, our goal was to build a machine-learning model capable of addressing the regression task of predicting the popularity scores of songs. We used a training dataset focused on Spotify songs from the past few decades; excluding ID and popularity, 13 attributes were included such as genre, release year, beats, loudness, and valance. These attributes guided the model-building process and facilitated effective predictions of song popularity on the testing dataset. A range of techniques was applied, from basic models for baseline comparisons (e.g. Linear, Lasso) to more sophisticated approaches (e.g. Random Forest, CatBoost)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf21829-4fc7-45b6-9b0d-77bc2357288a",
   "metadata": {},
   "source": [
    "### Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b0f2e87-7e9e-4e4d-b80a-bf7c292f2c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, PowerTransformer, MinMaxScaler, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, RFE\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84cfa1e-130e-46e0-8c73-8c29f76bf410",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a353dde9-d492-4238-92b6-d71f8949cb17",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304fd4f7-73a5-406a-88b4-d5db6aec47f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Reg_test = pd.read_csv(\"CS98XRegressionTest.csv\")\n",
    "Reg_train = pd.read_csv(\"CS98XRegressionTrain.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2bdadb-ca09-477c-b429-94fdd7af889f",
   "metadata": {},
   "source": [
    "Two datasets were used for regression to predict a song’s popularity score. The training set has 453 rows and 15 columns, including the target variable pop, and the identification variable, Id. The testing set has 114 rows and 14 columns, excluding pop for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1717a9-b682-40c2-8312-f75644ab497b",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900b3993-d85e-4dd6-8cde-4550629db113",
   "metadata": {},
   "source": [
    "## Missing value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662144ef-2528-4dad-a7a7-c0e00414a173",
   "metadata": {},
   "source": [
    "- **Top genre**: Found 15 missing values in the training set and 1 in the testing set. Missing data was replaced with \"Unknown\" as a separate category, by using Label Encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31218c3-1901-45a3-a03e-eda2f45d9bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Reg_train['top genre'] = Reg_train['top genre'].fillna('Unknown')\n",
    "Reg_test['top genre'] = Reg_test['top genre'].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfdce54-e077-412d-98b9-a9e874d9a664",
   "metadata": {},
   "source": [
    "## Skewness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9a4c0e-ee22-44d8-8b1a-43730e29d4e6",
   "metadata": {},
   "source": [
    "Skewness measures deviation from a normal distribution, impacting model performance. Highly skewed features can reduce prediction accuracy. We calculated skewness for numerical features: year, bpm, nrgy, dnce, dB, live, val, dur, acous, and spch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db505ac6-e9aa-403b-b266-9c2108b72399",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = Reg_train.select_dtypes(include=['number']).columns.drop(['pop', 'Id'])\n",
    "skewness_values = Reg_train[numerical_cols].skew().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaae2bb8-d559-4e30-908d-8d92fa411b1f",
   "metadata": {},
   "source": [
    "We found spch and live have Highly right-skewed and need Log transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ff8dc1-ae5b-441f-b701-ec1ece41f7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "highly_skewed_cols = skewness_values[skewness_values > 1.5].index\n",
    "Reg_train[highly_skewed_cols] = np.log1p(Reg_train[highly_skewed_cols])\n",
    "Reg_test[highly_skewed_cols] = np.log1p(Reg_test[highly_skewed_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025494eb-7f2a-4796-8df5-31d175a0609d",
   "metadata": {},
   "source": [
    "We found dur and acous have Moderately Skewed Features to handle skewness. We applied Cube Root Transformation to smooths out skewness without distorting data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f6acb5-b92f-486c-80a2-e0a2eb465fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "moderately_skewed_cols = skewness_values[(skewness_values > 0.5) & (skewness_values <= 1.5)].index\n",
    "Reg_train[moderately_skewed_cols] = np.cbrt(Reg_train[moderately_skewed_cols])\n",
    "Reg_test[moderately_skewed_cols] = np.cbrt(Reg_test[moderately_skewed_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3769d314-8c1f-4c85-a09e-41dd4cfaa1b6",
   "metadata": {},
   "source": [
    "Yeo-Johnson Transformation was applied to all numerical columns, regardless of skewness, to ensure a more stable and normally distributed dataset. Even if some columns appeared normally distributed, this transformation helped handle slight deviations and improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa209987-a7d6-4a67-b21c-9a0ea3d2bc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "Reg_train[numerical_cols] = pt.fit_transform(Reg_train[numerical_cols])\n",
    "Reg_test[numerical_cols] = pt.transform(Reg_test[numerical_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c580688-c064-4216-9e91-50bc53c3c6ea",
   "metadata": {},
   "source": [
    "## Encode top genre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb0190d-6c18-4f65-b6e7-59549d272388",
   "metadata": {},
   "source": [
    "The Top Genre feature is categorical, requiring encoding for the model to interpret it. Label Encoding was chosen for its lower memory usage compared to One-Hot Encoding. The encoder was fitted on known categories in the training set, while unseen categories in the test set were assigned -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b71ce7c-0575-4855-acad-6766469e1031",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "Reg_train['top genre_encoded'] = label_encoder.fit_transform(Reg_train['top genre'])\n",
    "Reg_test['top genre_encoded'] = Reg_test['top genre'].apply(lambda x: label_encoder.transform([x])[0] if x in label_encoder.classes_ else -1)\n",
    "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6843e7-81d9-484c-a8f9-491b80fb8a24",
   "metadata": {},
   "source": [
    "## Create new columns for year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824d3acc-8833-486f-9329-0f181216823c",
   "metadata": {},
   "source": [
    "The year column shows the release year of a song. We compute song_age by subtracting the current year and release year by assuming that newer songs may have different popularity trends compared to older songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9976b4cb-f582-4bc9-aa8a-cf2dd2e041e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_year = datetime.now().year\n",
    "Reg_train['song_age'] = current_year - Reg_train['year']\n",
    "Reg_test['song_age'] = current_year - Reg_test['year']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8df962f-5318-47ea-9dac-7eeba7a1518c",
   "metadata": {},
   "source": [
    "## Create new columns for artist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897503fc-c3b0-4c9b-b107-4db0a3e87cb6",
   "metadata": {},
   "source": [
    "Artist names cannot be used in ML. We suggest that more frequent artists might have a higher impact on popularity trends. Therefore, the artist_freq was created to represents how many times an artist appears in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfd8a36-6e60-4953-8544-5c406fb15a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_counts_train = Reg_train['artist'].value_counts()\n",
    "artist_counts_test = Reg_test['artist'].value_counts()\n",
    "Reg_train['artist_freq'] = Reg_train['artist'].map(artist_counts_train)\n",
    "Reg_test['artist_freq'] = Reg_test['artist'].map(artist_counts_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ced2e6-60ca-48c4-a55b-fe9727879089",
   "metadata": {},
   "source": [
    "We also convert it into numerical values using Label Encoding for using in ML. For unseen Artists in Reg_test, the -1 was assign to avoid errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3111cc0-cb47-46f4-8ef2-3d4cc6962ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "Reg_train['artist_encoded'] = label_encoder.fit_transform(Reg_train['artist'])\n",
    "Reg_test['artist_encoded'] = Reg_test['artist'].apply(lambda x: label_encoder.transform([x])[0] if x in label_encoder.classes_ else -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8067ec4-68d8-4500-b2bd-b82d9c4e978f",
   "metadata": {},
   "source": [
    "## Create new columns for Title "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19655294-6739-46f6-9ce2-cbca1c36b01b",
   "metadata": {},
   "source": [
    "The title column is text based to make this feature useful for ML models, we apply TF-IDF to captures word importance and reduces dimensionality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd0be16-4459-46e0-b1bf-1e3cd3b7979e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=50)\n",
    "title_tfidf_train = vectorizer.fit_transform(Reg_train['title'].astype(str)).toarray()\n",
    "title_tfidf_test = vectorizer.transform(Reg_test['title'].astype(str)).toarray() \n",
    "title_tfidf_train = pd.DataFrame(title_tfidf_train, columns=[f\"title_{i}\" for i in range(title_tfidf_train.shape[1])])\n",
    "title_tfidf_test = pd.DataFrame(title_tfidf_test, columns=[f\"title_{i}\" for i in range(title_tfidf_test.shape[1])])\n",
    "Reg_train = pd.concat([Reg_train.reset_index(drop=True), title_tfidf_train], axis=1)\n",
    "Reg_test = pd.concat([Reg_test.reset_index(drop=True), title_tfidf_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab87b1b-0282-47c5-8c38-5ca94cbfc5ec",
   "metadata": {},
   "source": [
    "## Check for outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35ec3ef-e267-434d-8086-a8efdf33d376",
   "metadata": {},
   "source": [
    "Outlier might distort relationships in the dataset. So, the IQR was used to detecting outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bbbb23-4755-4776-a796-4bfc0be0d0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['bpm', 'nrgy', 'dnce','dB', 'live', 'val', 'dur', 'acous', 'spch','song_age', 'artist_freq']\n",
    "def detect_outliers_iqr(df, columns):\n",
    "    outlier_summary = {}\n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)  \n",
    "        Q3 = df[col].quantile(0.75) \n",
    "        IQR = Q3 - Q1  \n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "        outlier_summary[col] = len(outliers)\n",
    "    return pd.DataFrame.from_dict(outlier_summary, orient='index', columns=['Outlier Count']).sort_values(by='Outlier Count', ascending=False)\n",
    "outlier_counts = detect_outliers_iqr(Reg_train, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1450a1ca-aea2-406e-8bcc-4eaf92731326",
   "metadata": {},
   "source": [
    "The result found that artist_freq has a large number (48) and dur(4), bpm(3), dB(2) also live (1) have a small number of outliers.\n",
    "While nrgy, dnce, val, acous, spch, and song_age don't have extreme values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de41eee-62df-4816-9702-72562f46b452",
   "metadata": {},
   "source": [
    "## Min-Max Scaling (0 to 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94460051-e863-4f85-a0c4-90dfb10a0072",
   "metadata": {},
   "source": [
    "Different types of features require different scaling techniques based on their distribution and range. Min-Max Scaling preserves the original distribution while bringing values between 0 and 1. This works best for features with a fixed range and no extreme outliers. Then 'nrgy', 'dnce', 'val', 'acous','spch', and 'song_age' are applied for Min-Max Scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133bdd98-e5f1-4952-8a61-1b8d4853ea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_cols = ['nrgy', 'dnce', 'val', 'acous','spch','song_age']\n",
    "minmax_scaler = MinMaxScaler()\n",
    "Reg_train[min_max_cols] = minmax_scaler.fit_transform(Reg_train[min_max_cols])\n",
    "Reg_test[min_max_cols] = minmax_scaler.transform(Reg_test[min_max_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b39aa16-b4fe-41ca-8c85-c98af73a8961",
   "metadata": {},
   "source": [
    "## Standardisation (Z-score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7539c1d8-1281-490d-974d-356d3848b2bc",
   "metadata": {},
   "source": [
    "When data has outliers, Z-score Standardization is useful for normally distributed or highly variable data. 'live', 'dur', 'artist_freq', and 'dB' are applied for Standardisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5137e9-426a-47e0-99e8-e83c282fec60",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardize_cols = ['live', 'dur', 'artist_freq','dB']\n",
    "standard_scaler = StandardScaler()\n",
    "Reg_train[standardize_cols] = standard_scaler.fit_transform(Reg_train[standardize_cols])\n",
    "Reg_test[standardize_cols] = standard_scaler.transform(Reg_test[standardize_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad7dfae-1419-4638-a34c-a4b26c676211",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42b8b09-0429-4d9d-9974-41eddd063746",
   "metadata": {},
   "source": [
    "In this feature selection step, pop is the target variable, we store it separately before removing it from Reg_train, ensuring we don’t lose it. Afterward, dropping Id which is an identifier that doesn't provide useful information. Moreover, the title, and artist are not directly usable, we use the encoded one instead. Lastly, the pop column is dropped because it is the target variable that is already stored separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aee37ee-bfd0-455a-87ff-a4f50274bc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Reg_train['pop']\n",
    "drop_columns_train = ['Id', 'title', 'artist', 'top genre', 'pop']\n",
    "drop_columns_test = ['title', 'artist', 'top genre']\n",
    "Reg_train.drop(columns=drop_columns_train, inplace=True)\n",
    "Reg_test.drop(columns=drop_columns_test, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f9aa52-aafb-4433-a09e-d06d3f63ef8c",
   "metadata": {},
   "source": [
    "## Check how useful of title Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005af0e6-895b-4e9c-b2f8-436a6c0698c2",
   "metadata": {},
   "source": [
    "The title column was transformed using TF-IDF, but not all features were useful. Variance analysis and correlation with pop were used to assess whether the encoded features had enough variation and impact on model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c894a1-a580-4d38-b284-01591dc9766d",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_features = [col for col in Reg_train.columns if 'title_' in col]\n",
    "Variance_analysis_output = Reg_train[title_features].var().sort_values()\n",
    "correlation_with_target = Reg_train[title_features].corrwith(y).abs().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70148617-f680-4272-9634-037e8285505d",
   "metadata": {},
   "source": [
    "- Features with very low variance (<0.01) provide little variability.\n",
    "- Features with a correlation >0.08 have stronger predictive power for pop.\n",
    "- title_0 (0.1006) and title_1 (0.1046) were kept due to their high correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8167a4-2c72-4e12-993d-58da0d84af35",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_title_features = [col for col in title_features if col not in ['title_0', 'title_1']]\n",
    "Reg_train.drop(columns=drop_title_features, inplace=True)\n",
    "Reg_test.drop(columns=drop_title_features, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5cb991-136c-448d-b2d9-b29db1adea96",
   "metadata": {},
   "source": [
    "Final Features\n",
    "- For train set: Contains 16 features excluding the target variable pop\n",
    "- Test Set: Contains 17 features, including Id as identifier. The Id column is present in Reg_test but not in Reg_train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c820256d-9a03-4c75-bd1a-647561434069",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb010b4a-bb8a-4aed-a605-402f30adaec6",
   "metadata": {},
   "source": [
    "Measures linear correlation between features & target (pop). This method is simple and interpretable for identifies relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51474020-6193-4790-8f33-e8dcc51e4cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Reg_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2afaf3-c40f-449e-8f3e-c791266ebabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features (Correlation Analysis): ['dur', 'acous', 'dB', 'nrgy', 'dnce', 'artist_freq', 'top genre_encoded', 'live', 'title_1', 'title_0']\n"
     ]
    }
   ],
   "source": [
    "correlation = X.corrwith(y).abs().sort_values(ascending=False)\n",
    "correlation_threshold = 0.1\n",
    "top_features_corr = correlation[correlation > correlation_threshold].index.tolist()\n",
    "print(\"Selected Features (Correlation Analysis):\", top_features_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217ee316-4761-4cc8-84fc-73500223a660",
   "metadata": {},
   "source": [
    "SelectKBest (f_regression) Uses ANOVA F-statistic to select features that best explain target variance. This method is good for linear relationships and Fast computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f4856a-c78c-42d3-8237-548bc62d9d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features (f_regression): Index(['nrgy', 'dnce', 'dB', 'live', 'dur', 'acous', 'top genre_encoded',\n",
      "       'artist_freq', 'title_0', 'title_1'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Select top 10 features based on f_regression\n",
    "selector = SelectKBest(score_func=f_regression, k=10)\n",
    "selector.fit(X, y)\n",
    "\n",
    "# Get selected feature names\n",
    "selected_features_f_reg = X.columns[selector.get_support()]\n",
    "print(\"Selected Features (f_regression):\", selected_features_f_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910804ad-b2f7-4dde-aec4-731b288d8988",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination (RFE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edcf090-e99f-4242-b822-4be26787ea1a",
   "metadata": {},
   "source": [
    "Recursive Feature Elimination (RFE) works by iteratively removing the least important features using a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da4fd25-b7cd-42c0-b80f-45b8da2b57d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features (RFE): Index(['year', 'bpm', 'nrgy', 'dB', 'dur', 'acous', 'top genre_encoded',\n",
      "       'song_age', 'artist_freq', 'artist_encoded'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rfe = RFE(rf, n_features_to_select=10)\n",
    "rfe.fit(X, y)\n",
    "selected_features_rfe = X.columns[rfe.support_]\n",
    "print(\"Selected Features (RFE):\", selected_features_rfe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31de570e-0e55-4e96-a10d-1ca9c785e439",
   "metadata": {},
   "source": [
    "Random Forest Feature Importance Uses Random Forest to rank features based on how much they reduce prediction error. This method is good for handling non-linear relationships and works well for high-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e1a494-d867-4068-9e64-7baa7e5e5284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features (Random Forest Importance): ['dur', 'acous', 'nrgy', 'song_age', 'year', 'dB', 'artist_freq', 'top genre_encoded', 'bpm', 'artist_encoded']\n"
     ]
    }
   ],
   "source": [
    "# Train Random Forest and get feature importances\n",
    "rf.fit(X, y)\n",
    "feature_importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "\n",
    "# Select top 10 features\n",
    "top_features_rf = feature_importances.nlargest(10).index.tolist()\n",
    "print(\"Selected Features (Random Forest Importance):\", top_features_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b2e971-e002-4f77-bb61-524efd3afaf4",
   "metadata": {},
   "source": [
    "## Lasso Regression Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed63477a-cd48-48fa-9f27-d52a57a58802",
   "metadata": {},
   "source": [
    "Lasso Regression Feature Selection Uses L1 regularisation to shrink less important feature coefficients to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c119daf2-fb34-46af-a802-3b9042b5c5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features (Lasso Regression): Index(['year', 'bpm', 'nrgy', 'dnce', 'dB', 'live', 'val', 'dur', 'acous',\n",
      "       'top genre_encoded', 'song_age', 'artist_freq', 'artist_encoded',\n",
      "       'title_0', 'title_1'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_scaled, y)\n",
    "selected_features_lasso = X.columns[lasso.coef_ != 0]\n",
    "print(\"Selected Features (Lasso Regression):\", selected_features_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e741d9b7-bd77-4ec0-b20b-c55f29d093a8",
   "metadata": {},
   "source": [
    "## Compare Feature Selection Methods "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a19c5d-28e8-44f1-a3f4-c34526d0306e",
   "metadata": {},
   "source": [
    "After apply 16 Features to all methods above to determine the most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6bb1a5-1617-4d9b-bc68-41936395f177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features (common features): {'dB', 'acous', 'top genre_encoded', 'dur', 'nrgy', 'artist_freq'}\n"
     ]
    }
   ],
   "source": [
    "common_features = set(top_features_corr) & set(selected_features_rfe) & set(selected_features_lasso)\n",
    "print(\"Selected Features (common features):\", common_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db21db96-1b6a-4660-8305-48bd6500519e",
   "metadata": {},
   "source": [
    "Common Features is the most important feature and consistent across methods "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126c61cc-edee-4804-adf1-d18c7c5f7dc3",
   "metadata": {},
   "source": [
    "There are 4 metric were used to evaluate. There are MAE (Mean Absolute Error), MSE (Mean Squared Error),RMSE (Root Mean Squared Error), andR² Score (Coefficient of Determination)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd2c664-70af-445b-85d6-93095e10d7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_feature_selection(X, y, feature_sets):\n",
    "    results = []  \n",
    "    for method, features in feature_sets.items():\n",
    "        valid_features = list(features.intersection(X.columns))\n",
    "        if not valid_features:\n",
    "            continue \n",
    "        X_selected = X[valid_features]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "        model = RandomForestRegressor(random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        results.append({\n",
    "            \"Feature Selection Method\": method, \"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R² Score\": r2\n",
    "        })  \n",
    "    return pd.DataFrame(results)\n",
    "feature_sets = {\n",
    "    \"Correlation Analysis\": set(top_features_corr),\"Recursive Feature Elimination (RFE)\": set(selected_features_rfe),\n",
    "    \"Lasso Regression\": set(selected_features_lasso),\"Common Features (Multiple Methods)\": set(common_features)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98b9393-ee29-44dd-9285-0085b7e2e7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = evaluate_feature_selection(X, y, feature_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f11614-9c4a-45ac-a0e4-a5ffd13fb0e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Selection Method</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R² Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Correlation Analysis</td>\n",
       "      <td>8.609231</td>\n",
       "      <td>122.813398</td>\n",
       "      <td>11.082121</td>\n",
       "      <td>0.450936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Recursive Feature Elimination (RFE)</td>\n",
       "      <td>8.429890</td>\n",
       "      <td>114.620719</td>\n",
       "      <td>10.706107</td>\n",
       "      <td>0.487563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>8.566703</td>\n",
       "      <td>114.516696</td>\n",
       "      <td>10.701247</td>\n",
       "      <td>0.488028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Common Features (Multiple Methods)</td>\n",
       "      <td>8.569011</td>\n",
       "      <td>122.946719</td>\n",
       "      <td>11.088134</td>\n",
       "      <td>0.450340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Feature Selection Method       MAE         MSE       RMSE  \\\n",
       "0                 Correlation Analysis  8.609231  122.813398  11.082121   \n",
       "1  Recursive Feature Elimination (RFE)  8.429890  114.620719  10.706107   \n",
       "2                     Lasso Regression  8.566703  114.516696  10.701247   \n",
       "3   Common Features (Multiple Methods)  8.569011  122.946719  11.088134   \n",
       "\n",
       "   R² Score  \n",
       "0  0.450936  \n",
       "1  0.487563  \n",
       "2  0.488028  \n",
       "3  0.450340  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576eee45-5e2c-4816-a7c0-8c196f9c12df",
   "metadata": {},
   "source": [
    "- The best Feature Selection Methods is Recursive Feature Elimination (RFE) which has the Lowest RMSE (10.60)and the Highest R² (0.4970).\n",
    "- The worst Feature Selection Methods is Correlation Analysis with Highest RMSE (11.13) and Lowest R² Score (0.4461)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d76d9f5-41ee-4a75-9d1d-cae32629075e",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc16e80b-52e1-448d-9a00-7728135d70d7",
   "metadata": {},
   "source": [
    "Final features to use in model are bpm, dur, dB, top genre_encoded, song_age, year, artist_freq, artist_encoded, acous, and nrgy from RFE for better model performance. Then, split into train and test sets to train the model on one part and evaluate it on another to avoid overfitting and use random_state=42, to ensure consistent results every time the code runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4192522b-49da-406f-9771-447cfd8ba237",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Reg_train[selected_features_rfe]\n",
    "X_test_final = Reg_test[selected_features_rfe]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c670289-267f-4777-ae3e-1e8f00c64a71",
   "metadata": {},
   "source": [
    "### Evaluation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e81d0b9-0ce5-4b71-9adc-e311258e94f7",
   "metadata": {},
   "source": [
    "The key evaluation metrics were calculated to evaluate the models. There are MAE (Mean Absolute Error), MSE (Mean Squared Error), RMSE (Root Mean Squared Error), and R² Score (Coefficient of Determination)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75a0d76-dad6-4ebe-a014-50a1424de492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    result = pd.DataFrame([{\"Model\": model_name,\"MAE\": mae,\"MSE\": mse,\"RMSE\": rmse,\"R² Score\": r2}])\n",
    "    print(f\"\\n Performance of {model_name}:\")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beabd7c-f69b-4711-87dd-e421285c473d",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e91437-4294-4e9d-b398-ba8cb868db10",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c62d18f-e749-436d-9665-7d89b019b156",
   "metadata": {},
   "source": [
    "Linear Regression is the simplest model that assumes a linear relationship between features and target. It is fast and easy to interpret but Fails on non-linear data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87ef7a8-3850-4905-b4ad-241d7d2a55e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Performance of Linear Regression:\n",
      "               Model       MAE         MSE       RMSE  R² Score\n",
      "0  Linear Regression  9.454644  134.907251  11.614958  0.396868\n"
     ]
    }
   ],
   "source": [
    "lr_model = LinearRegression()\n",
    "evaluate_model(lr_model, X_train, X_test, y_train, y_test, \"Linear Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175f3bce-dd1c-4fb7-8e14-7b3976272b3d",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1903c1-6a08-479e-b2d5-aaf8ae8a8c08",
   "metadata": {},
   "source": [
    "Lasso Regression is similar to Ridge, but also performs feature selection by shrinking some coefficients to zero. This model prevents overfitting but performs poorly if features are highly correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cb2d41-832a-4eca-927a-9d5bc458b4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Performance of Lasso Regression:\n",
      "              Model       MAE         MSE       RMSE  R² Score\n",
      "0  Lasso Regression  9.475763  134.905331  11.614875  0.396877\n"
     ]
    }
   ],
   "source": [
    "lasso_model = Lasso(alpha=0.1)\n",
    "evaluate_model(lasso_model, X_train, X_test, y_train, y_test, \"Lasso Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd36c9e1-1a79-4e06-93c7-a883bddf72b1",
   "metadata": {},
   "source": [
    "### Decision Tree Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09500f38-020c-4687-80a7-aec138ad0f7f",
   "metadata": {},
   "source": [
    "Decision Tree Regression is a simple tree-based model that captures non-linear relationships in data. This model Handles both linear & non-linear data and works well with missing values. However,it is overfits easily.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0552612-117d-467b-8da9-778836a4f573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Performance of Decision Tree Regression:\n",
      "                      Model        MAE         MSE       RMSE  R² Score\n",
      "0  Decision Tree Regression  11.406593  209.054945  14.458732  0.065375\n"
     ]
    }
   ],
   "source": [
    "dt_model = DecisionTreeRegressor(random_state=42)\n",
    "evaluate_model(dt_model, X_train, X_test, y_train, y_test, \"Decision Tree Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f522cf9-0d9f-494f-a082-e406fb1d65fa",
   "metadata": {},
   "source": [
    "## Advance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc9afc4-aeaa-46eb-bc70-dd334db3c26e",
   "metadata": {},
   "source": [
    "### Random Forest Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5665e78e-1e8d-4515-be6a-2200e7f8d6fd",
   "metadata": {},
   "source": [
    "Random Forest Regression is an ensemble of decision trees that reduces overfitting by averaging multiple predictions, it handles non-linearity well and is less sensitive to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8fa0bd-5233-4a7c-b04b-e53a99aefe3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Performance of Random Forest Regression:\n",
      "                      Model       MAE         MSE       RMSE  R² Score\n",
      "0  Random Forest Regression  8.408846  114.088122  10.681204  0.489944\n"
     ]
    }
   ],
   "source": [
    "rf_model = RandomForestRegressor(n_estimators=200, random_state=42)\n",
    "evaluate_model(rf_model, X_train, X_test, y_train, y_test, \"Random Forest Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9ec2fb-f28d-4d33-bb72-c5ca73d2127c",
   "metadata": {},
   "source": [
    "### Gradient Boosting Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4a432e-b229-4eaf-9834-f5351e284e78",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression sequentially builds trees, focusing on correcting previous errors to improve accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e6c63a-6e76-4f43-b6e4-ee89db426d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Performance of Gradient Boosting Regression:\n",
      "                          Model       MAE         MSE       RMSE  R² Score\n",
      "0  Gradient Boosting Regression  8.449046  109.791038  10.478122  0.509155\n"
     ]
    }
   ],
   "source": [
    "gb_model = GradientBoostingRegressor(n_estimators=200, random_state=42)\n",
    "evaluate_model(gb_model, X_train, X_test, y_train, y_test, \"Gradient Boosting Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5631b52-360b-4b2e-9a1d-29b4ec409524",
   "metadata": {},
   "source": [
    "### CatBoost Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a22321-35c2-4ba7-95bc-3db1352e6660",
   "metadata": {},
   "source": [
    "CatBoost Regression is a Gradient Boosting model optimized for categorical data. It required less hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e87cfdc-02b9-4aaa-910d-3b8f8c71c178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Performance of CatBoost Regression:\n",
      "                 Model       MAE       MSE      RMSE  R² Score\n",
      "0  CatBoost Regression  7.898911  96.79184  9.838284  0.567271\n"
     ]
    }
   ],
   "source": [
    "cat_model = CatBoostRegressor(iterations=200, learning_rate=0.1, depth=5, verbose=0, random_state=42)\n",
    "evaluate_model(cat_model, X_train, X_test, y_train, y_test, \"CatBoost Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f39264-139c-4834-ad66-3c8d5e9be55d",
   "metadata": {},
   "source": [
    "### Stacking Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5232477-ff04-4c0a-ad57-225166f83043",
   "metadata": {},
   "source": [
    "Stacking Regression is a meta-learning approach that combines multiple models to improve performance. This can leverage multiple models' strengths, however is complex to implement and is computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f4ded2-4670-414c-86c1-4b7a9ca927ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Performance of Stacking Regression:\n",
      "                 Model       MAE        MSE      RMSE  R² Score\n",
      "0  Stacking Regression  7.887805  99.633482  9.981657  0.554567\n"
     ]
    }
   ],
   "source": [
    "base_models = [\n",
    "    ('rf', RandomForestRegressor(n_estimators=200, random_state=42)),\n",
    "    ('gb', GradientBoostingRegressor(n_estimators=200, random_state=42)),\n",
    "    ('xgb', xgb.XGBRegressor(n_estimators=200, learning_rate=0.1, max_depth=5, random_state=42)),\n",
    "    ('cat', CatBoostRegressor(iterations=200, learning_rate=0.1, depth=5, verbose=0, random_state=42))\n",
    "]\n",
    "meta_model = Ridge(alpha=1.0)\n",
    "stacking_model = StackingRegressor(estimators=base_models, final_estimator=meta_model)\n",
    "evaluate_model(stacking_model, X_train, X_test, y_train, y_test, \"Stacking Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40993e7-2a5d-4955-8b45-68fd1917132f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_final = Reg_test[selected_features_rfe]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02364ed0-ac06-4441-a5ff-6031b2cdba4d",
   "metadata": {},
   "source": [
    "### Final Model and Implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c5dc2d2-ba6d-4b54-80a7-9ad19d15c004",
   "metadata": {},
   "source": [
    "As a result, the Lasso regressor performs as a strong baseline model, whereas CatBoost delivers superior performance for challenging predictions. Lasso regression produced the lowest RMSE of 11.61 and an R^2 score of 0.396 amongst other baseline models. It acts as an effective starting point as it identifies important features in predicting song popularity while minimising overfitting. However, it may not capture the complexity of data patterns. \n",
    "\n",
    "Therefore, a final advanced model was chosen - CatBoost (Categorical Boosting) Regressor. It is an ensemble learning approach built on a gradient-boosting algorithm that combines multiple models to handle categorical data efficiently. It was chosen because it produced the lowest RMSE of 9.83 and an R^2 score of 0.567 compared to other advanced models, effectively capturing patterns in the dataset while reducing prediction errors. A mix of numerical attributes such as loudness and energy and categorical features such as top genre predicts song popularity. CatBoost’s ability to automatically handle different data types and its strong performance metrics allowed a more streamlined and accurate model. \n",
    "\n",
    "A further reason the CatBoost model might have performed so well in comparison to baseline models is its ability to handle non-linear data. Some of the variables within the dataset are non-linear such as danceability or energy, meaning that simpler models like LASSO regression struggle to provide accurate predictions.\n",
    "\n",
    "Once the Lasso and CatBoost models were trained, it was then applied to the test dataset to produce final predictions of the song's popularity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74b72ba",
   "metadata": {},
   "source": [
    "- The best model as the baseline model is Lasso Regression with lowest RMSE(11.6149) with highest R² (0.3969)\n",
    "- The best model as the advance model is CatBoost Regression with lowest RMSE(9.8383) with highest R² (0.5673) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbbcb58-a187-4d89-aca0-5d4a9722d3f6",
   "metadata": {},
   "source": [
    "### Lasso Regression is the best model for Basic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4c8fdc-991c-48c1-90b4-3f823b97656b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model.fit(X_train, y_train)\n",
    "y_pred_test = lasso_model.predict(X_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d220714-aa4e-4399-ae35-38d3c85a67c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df_lasso_model = pd.DataFrame({\"Id\": Reg_test[\"Id\"],  \"pop\": y_pred_test })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73624da0-974c-4e94-a951-58a846e72649",
   "metadata": {},
   "source": [
    "### CatBoost Regression is the best model for Advanced "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4290c842-78f2-4035-bd58-87d52af27411",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_model.fit(X_train, y_train)\n",
    "y_pred_test = cat_model.predict(X_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d069e8a-1eba-4659-ab5c-ea3016d3f26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df_cat_model = pd.DataFrame({ \"Id\": Reg_test[\"Id\"], \"pop\": y_pred_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515a98c6",
   "metadata": {},
   "source": [
    "## Insights from the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735d0ab2",
   "metadata": {},
   "source": [
    "As it has been outlined, the CatBoost model was the best-performing model created for this project. Its low RMSE means that the predicted values from the model were very similar to the actual values provided with the model not overfitting and maintaining generalisation. As well as the tuning of the models through the use of methods such as feature selection  we were further able to improve the predictive capabilities of our regression model. More specifically the feature selection processes such as Recursive Feature Elimination (RFE) and Random Forest Importance were proven to be the most effective with RMSE scores of 10.54 respectively. \n",
    "\n",
    "The CatBoost Regressor outperformed the baseline models with an RMSE of 9.84 and an improved RMSE of 7.5 on the Kaggle test dataset. This demonstrates that our model generalised very well to the unseen samples based on this reduction in RMSE. The model successfully captured patterns from the main dataset proving that our steps taken to build the model ie feature selection as well as model selection were successful. However, it should be noted that this RMSE score from the Kaggle dataset is only based on 50% of the test data. The final result may vary from this 7.5 as the entire dataset was not fully utilised. Nonetheless, a positive outcome overall. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1e4467",
   "metadata": {},
   "source": [
    "## Concluding Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd514e6",
   "metadata": {},
   "source": [
    "Overall, the CatBoost regression model created was very successful in predicting the popularity score of a song. The feature selection process showed that the RFE and Random Forest Importance were the most effective at helping narrow down the attribute selection. Not only this but the process of trialling several different regression models and comparing their RMSE score allowed for a wide range of models to be analysed and broken down. These two main processes undertaken proved to be very effective as the RMSE score from the Kaggle leaderboard was 7.50631, much lower than the validation set meaning our model fit the data very well with minimal overfitting. \n",
    "\n",
    "A downside to this process was the time taken to create all these separate models. The selection of the models and subsequent fine-tuning of them was a very time-consuming process with the majority of the models not even being used in the end when their RMSE score was calculated to be high. It would have been much easier and faster to simply create one model from the start and stick with it. However, we would not have been able to compare different models and thus perhaps create a sub-optimal algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
